{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97562086-ded2-42e4-9d41-34784a9bc2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ccf50-80c7-49f5-b44b-76f43c351a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model_name = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"model_with_data\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps = 14869,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65a7b1-14f9-4240-bad5-9cdc1d306d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf86f5-ad5e-492c-911b-d27a5a31d7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1519f11-6f44-4eee-b66a-1e03c0af020a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97663f14-234c-40f8-b847-36412e2f3189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re, string, pickle\n",
    "#re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): \n",
    "    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "class Judge():\n",
    "    def __init__(self):\n",
    "        with open('judge_model/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "            self.vec = pickle.load(file)\n",
    "        with open('judge_model/logistic_regression_model.pkl', 'rb') as file:\n",
    "            self.model = pickle.load(file)\n",
    "        with open('judge_model/r_values.pkl', 'rb') as file:\n",
    "            self.r = pickle.load(file)\n",
    "            \n",
    "    def judge(self, words):\n",
    "        tokens = self.vec.transform(words)\n",
    "        preds = self.model.predict_proba(tokens.multiply(self.r))[:,1]\n",
    "        \n",
    "        return 1 if preds>0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06138386-544f-45a9-98ec-d436d0eea3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d1f73-e5a3-4e81-8972-f8eeb0ec7088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='dirty.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b2d9a-2b1e-4343-a005-faf848dc16ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    text = examples['comment_text']\n",
    "    examples['input_ids'] = tokenizer(\"paraphrase: \" + text, max_length=128, truncation=True)['input_ids']\n",
    "    # example['labels'] = tokenizer(text_target=neutral1, max_length=max_target_length, truncation=True)['input_ids']\n",
    "    # new_examples['labels'].append(labels['input_ids'])\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=['id','comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b57c78-6ec4-4a7c-b9e1-0dd65464d56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272df2a-7491-4f3b-9a14-9c0c4d0ba2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTrainer(Seq2SeqTrainer):\n",
    "\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                ref_model,\n",
    "                args,\n",
    "                judge=None,\n",
    "                train_dataset=None,\n",
    "                eval_dataset=None,\n",
    "                data_collator=None,\n",
    "                tokenizer=None):\n",
    "        super().__init__(model,\n",
    "                args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer)\n",
    "        self.judge = judge\n",
    "        self.total_true = 0\n",
    "        self.ref_model = ref_model.to(model.device)\n",
    "    \n",
    "    def toxic_score(self, model, sent, w_answer, w_label):\n",
    "        logits = model(input_ids=sent, decoder_input_ids=w_answer).logits\n",
    "        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=w_label.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return per_token_logps.sum(-1)\n",
    "        \n",
    "    def clean_score(self, model, sent, c_answer, c_label):\n",
    "        logits = model(input_ids=sent, decoder_input_ids=c_answer).logits\n",
    "        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=c_label.unsqueeze(2)).squeeze(2)\n",
    "        return per_token_logps.sum(-1)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        new_inputs = model.generate(inputs.input_ids, max_new_tokens=20)\n",
    "        words = self.tokenizer.batch_decode(new_inputs, skip_special_tokens=True)\n",
    "        result = self.judge.judge(words)\n",
    "        labels = new_inputs.clone()\n",
    "        if random.random() >=0.9:\n",
    "            result = result^1\n",
    "        if result == 1:\n",
    "            # this works for only toxic words\n",
    "            policy_rejected_logps = self.toxic_score(model, inputs.input_ids, new_inputs, labels) # original sentence, wrong answer, wrong labels\n",
    "            reference_rejected_logps = self.toxic_score(self.ref_model, inputs.input_ids, new_inputs, labels) \n",
    "            \n",
    "            pi_logratios = 0 - policy_rejected_logps\n",
    "            ref_logratios = 0 - reference_rejected_logps\n",
    "            logits = pi_logratios - ref_logratios\n",
    "\n",
    "            loss = (\n",
    "                - F.logsigmoid(0.1 * logits) * (1 - 0.1)\n",
    "                - F.logsigmoid(-0.1 * logits) * 0.1\n",
    "            ).mean()\n",
    "        elif result == 0:\n",
    "            # this works for clean words\n",
    "            self.total_true += 1\n",
    "            policy_chosen_logps = self.clean_score(model, inputs.input_ids, new_inputs, labels) # original sentence, correct answer, correct labels\n",
    "            policy_rejected_logps = self.toxic_score(model, inputs.input_ids, inputs.input_ids.clone(), inputs.input_ids.clone()) #original sentence, wrong answer, wrong labels\n",
    "            reference_chosen_logps = self.clean_score(self.ref_model, inputs.input_ids, new_inputs, labels) # original sentence, correct answer, correct labels\n",
    "            reference_rejected_logps = self.toxic_score(self.ref_model, inputs.input_ids, inputs.input_ids.clone(), inputs.input_ids.clone()) #original sentence, wrong answer, wrong labels    \n",
    "            \n",
    "            pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "            ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "            logits = pi_logratios - ref_logratios\n",
    "            \n",
    "            loss = (\n",
    "                - F.logsigmoid(0.1 * logits) * (1 - 0.1)\n",
    "                - F.logsigmoid(-0.1 * logits) * 0.1\n",
    "            ).mean()\n",
    "            \n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "    \n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "        print('number of clean words: ', self.total_true)\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd1152-b90c-4034-827b-a5aeaea1f021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge = Judge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aa72f-5ccd-42e4-8670-983ef377161f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args,\n",
    "    judge = judge,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74b3d2-fe70-4f7e-aabd-3793fdf8af75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 8330 16613 25134 33497 41799 50192 58576 67011 75420 83766 92114 100498 108912 117328 125750\n",
    "# 8330 8230  8521  8363  8302  8393  8384  8435  8409  8346  8348  8384   8416   8416   8422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3572a-0115-43e0-b4c4-33eb369c78f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = \"you are a stupid fuck your mother's cunt stinks\"\n",
    "token = tokenizer(\"paraphrase: \" + words, return_tensors=\"pt\")\n",
    "\n",
    "new_inputs = model.generate(token.input_ids.to(model.device))\n",
    "print(tokenizer.batch_decode(new_inputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f9ced-2e1e-4153-a43c-4f52461c95c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"model_with_dpo_90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1104f997-9ead-4486-8bc8-335ef20a9584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7ab63d-e2fe-41d2-a974-bd632582f470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 14869\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bd643-5a4c-417c-981e-5534a8abfb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10965\n"
     ]
    }
   ],
   "source": [
    "test = AutoModelForSeq2SeqLM.from_pretrained('humarin/chatgpt_paraphraser_on_T5_base').to('cuda')\n",
    "\n",
    "count = 0\n",
    "for i in range(len(dataset)):\n",
    "    new_inputs = test.generate(torch.tensor([dataset[i]['input_ids']]).to(test.device))\n",
    "    words = tokenizer.batch_decode(new_inputs, skip_special_tokens=True)\n",
    "    result = judge.judge(words)\n",
    "    if result==0:\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871d57c3-7665-449e-bda4-208f34f3d7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() got an unexpected keyword argument 'max_new_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 5\u001b[0m     new_inputs \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(my_model\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m      6\u001b[0m     words \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(new_inputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     result \u001b[38;5;241m=\u001b[39m judge\u001b[38;5;241m.\u001b[39mjudge(words)\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() got an unexpected keyword argument 'max_new_tokens'"
     ]
    }
   ],
   "source": [
    "my_model = AutoModelForSeq2SeqLM.from_pretrained('model_with_dpo_second').to('cuda')\n",
    "\n",
    "count = 0\n",
    "for i in range(len(dataset)):\n",
    "    new_inputs = my_model.generate(torch.tensor([dataset[i]['input_ids']], max_new_tokens=20).to(my_model.device))\n",
    "    words = tokenizer.batch_decode(new_inputs, skip_special_tokens=True)\n",
    "    result = judge.judge(words)\n",
    "    if result==0:\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc7bbd1-792a-4f91-a793-61ba166764dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5124784349816838\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf241e7f-c48c-4356-8bb4-993d93f908d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "print(a^1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1611f-6b3e-42b0-8395-a20aab8253e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
