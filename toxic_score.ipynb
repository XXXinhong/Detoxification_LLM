{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa3312f-1a36-430c-8707-076633fc1430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21d58d6-14cc-4749-adfe-fdb79eeed69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='paradetox.tsv', delimiter='\\t', split='train')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('model_with_dpo_onlypass')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    text = examples['toxic']\n",
    "    examples['input_ids'] = tokenizer(\"paraphrase: \" + text, max_length=128, truncation=True)['input_ids']\n",
    "    # example['labels'] = tokenizer(text_target=neutral1, max_length=max_target_length, truncation=True)['input_ids']\n",
    "    # new_examples['labels'].append(labels['input_ids'])\n",
    "    return examples\n",
    "\n",
    "# dataset = dataset.map(preprocess_function, remove_columns=['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562514e2-cce0-4fce-858e-98325a367d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['toxic', 'neutral1', 'neutral2', 'neutral3'],\n",
      "    num_rows: 11927\n",
      "})\n",
      "Dataset({\n",
      "    features: ['toxic', 'input_ids'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=['neutral1', 'neutral2', 'neutral3']).select(range(500))\n",
    "print(dataset)\n",
    "# dataset = dataset.map(preprocess_function, remove_columns=['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6378b-c347-40cd-ad91-f21a4e297b35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'model_with_dpo_second'\n",
    "model_name = 'model_with_dpo_onlypass'\n",
    "# model_name = 'humarin/chatgpt_paraphraser_on_T5_base'\n",
    "\n",
    "# my_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "my_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "input_texts = []\n",
    "for i in range(len(dataset)):\n",
    "    new_inputs = my_model.generate(torch.tensor([dataset[i]['input_ids']]).to(my_model.device))\n",
    "    words = tokenizer.batch_decode(new_inputs, skip_special_tokens=True)\n",
    "    input_texts.append(dataset[i]['toxic'])\n",
    "    input_texts.append(words[0])\n",
    "    # result = judge.judge(words)\n",
    "    # if result==0:\n",
    "    #     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8bc53-b28b-4cac-91ba-e5f85aee513c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old = input_texts[0::2]\n",
    "new = input_texts[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26b862-c28a-44eb-b739-185a97caf19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# with open('test_result_two/model_with_dpo.csv', 'w', newline='') as file:\n",
    "with open('model_with_dpo_onlypass.csv', 'w', newline='') as file:\n",
    "# with open('test_result_two/based_model.csv', 'w', newline='') as file:\n",
    "    \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['old', 'new'])\n",
    "    # Optional: If you want each string in its own row\n",
    "    for i in range(len(old)):  # Loop through the length of the first column\n",
    "\n",
    "        writer.writerow([old[i], new[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd4f983-1e70-4fd1-9783-1f6dda02df5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccc8e07c-5bb0-470c-a849-93eb60c2735a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# load tokenizer and model weights\n",
    "tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "\n",
    "# prepare the input\n",
    "# batch = tokenizer.encode('you are amazing', return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b05bf60d-dc5b-4414-96a9-cdc38b8c9519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7c425f2dcb4681b884656348f8e53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3b4e1214254fddad0aac7c33f4859f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d0f6082d8b4f49b6c1ac358c5f7155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='test_result_two/model_with_dpo.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2d3550-efcc-495f-86f6-efe57d5184dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['old', 'new'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7edff1e-ce95-4605-affe-1d7fe8d822d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "\n",
    "for i in range(len(dataset['new'])):\n",
    "    batch = tokenizer.encode(dataset['new'][i], return_tensors='pt')\n",
    "    result = model(batch)\n",
    "    if torch.softmax(result.logits[0], dim=0)[0] > 0.8:\n",
    "        sum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69fc8ff6-4612-4bda-b393-d38d852b272d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456\n"
     ]
    }
   ],
   "source": [
    "print(sum/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0902974-5399-4b95-986c-f0b252dc76df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = tokenizer.encode(dataset['old'][0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a23909-9137-4c8f-b8fd-cc05fe550f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55f3cd2c-12f2-467f-b5ee-98c77f64735d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3960,  2.3409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d51910-9eeb-4817-a7f9-ee33de5e9e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(result.logits[0], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddde7fc0-4168-4fd3-882f-501975a1de66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example texts\n",
    "# reference = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']]\n",
    "# candidate = ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "dataset = load_dataset('csv', data_files='test_result_two/model_with_data.csv', split='train')\n",
    "\n",
    "# Calculate BLEU score\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "# print(\"BLEU score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6c2c508-4db8-4009-bae5-ee796cc26aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.7259795291154771\n"
     ]
    }
   ],
   "source": [
    "reference = [['thequick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "# Calculate BLEU score\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(\"BLEU score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2b39d9e-c130-4bd8-8a13-2b09f01c747f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = dataset['old']\n",
    "candidate = dataset['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c6c5cb4-5658-4e75-9fcb-a04d652a684c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.43348976166702\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sum += sentence_bleu([reference[i]], candidate[i])\n",
    "    \n",
    "print(sum/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cdd3f8-4942-4ce9-ac3f-88cfc7c2af47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
